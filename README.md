# [Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement] ğŸš€


This is the official PyTorch implementation for our paper **"Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement"**.

In this project, we tackle the problem of [Problem You Solved, e.g., temporal variance in cross-view geo-localization]. We propose a novel architecture named `[Your_Method_Name]`, which achieves... [Main advantage of your method]... ğŸ§ .

---

## ğŸŒŸ Highlights

* **[Highlight 1]**: e.g., Propose a new XX loss function, boosting accuracy by XX%.
* **[Highlight 2]**: e.g., Clean, reproducible, and easily extensible code.
* **[Highlight 3]**: e.g., Release a new large-scale dataset `[Dataset_Name]` for XX task.

---

## ğŸ›ï¸ Core Architecture

The overall architecture of our model `[Your_Method_Name]` is illustrated below:

![Model Architecture](assets/architecture.png)
*Figure 1: An overview of [Your_Method_Name].*

---

## âš™ï¸ Installation

We recommend using `conda` or `venvv` to create a virtual environment.

1.  **Clone this repository:**
    ```bash
    git clone
````ps://github.com/](https://github.com/)[Your_Username]/[Your_Repo_Name].git
    cd [Your_Repo_Name]
    ```

2.  **Create environment and install dependencies:**
```````bash
    # (Optional) Using Conda
    conda create -n [your_env_name] python=3.9
````conda activate [your_env_name]
    
    # Install dependencies
    pip install -r requirements.txt
    ```

3.
````repare datasets:**
    Please download the dataset from [Link to dataset] and unzip it into the `./data` directory.

---

## ğŸš€ How
````se

We provide complete scripts for training and evaluation.

### 1. Training
```bash
# Train your model
python train.py --config configs/
````nfig.yaml --data_path ./data/



<h1 align="center"> UniV-Baseline </h1>

<h1 align="left"> Download UniV</h1>

![image-20250730152824047](./assets/bubble_all.png)


<h1 align="center"> Click the image to the visualization of UniV</h1>

**Task 1: *Video-based* drone-view target localization.** (Drone-video -> Satellite-image) Given one drone-view video, the task aims to find the most similar satellite-view image to localize the target building in the satellite view. 

**Task 2: *Video-based* Drone navigation.** (Satellite-image -> Drone-video) Given one satellite-view image, the drone intends to find the most relevant place that it has passed by.

[BaiduCloud](https://pan.baidu.com/s/1fTEN3E2V82tia0JKAoTkrw?pwd=4g47)|

## TODOs

- [x] ~~(Optional) Release the 2-fps BEVs for both training and evaluation~~
- [x] ~~Release the `requirements.txt`~~
- [x] ~~Release the UniV dataset~~
- [x] ~~Release the weight of the second stage~~
- [x] ~~Release the ***evaluation*** code for the second stage~~
- [x] ~~Release the **training** code for the second stage~~
- [x] ~~Release the weight of the first stage~~
- [x] ~~Release the ***evaluation*** code for the first stage~~
- [x] ~~Release the **training** code for the first stage~~

<h1 align="left"> Table of contents</h1>

- [Dataset Introduction](#About-Dataset)
- [Getting started](#Getting-started)
- [Dataset & Preparation](#Dataset-&-Preparation)
- [Training & Evaluation](#Training-&-Evaluation)
- [Weights](#Weights)
- [Citation](#Citation)

## About Dataset

![image-20250730152824047](./assets/image-20250730152824047.png)

The dataset split is as follows: 
| Split  for the each subset | #data | #buildings | #universities|
| --------   | -----  | ----| ----|
|Training | 701 **vids** + 12364 imgs | 701 | 33 |
| Query_drone | 701 **vids** | 701 |  39 |
| Query_satellite | 701 imgs | 701 | 39|
| Query_ground | 2,579 imgs | 701 | 39|
| Gallery_drone | 951 **vids** | 951 | 39|
| Gallery_satellite | 951 imgs | 951 | 39|
| Gallery_ground | 2,921 imgs | 793  | 39|

More detailed file structure:

```bash
.
â”œâ”€â”€ 30
â”‚Â Â  â”œâ”€â”€ 10fps
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ gallery_drone
â”‚Â Â  â”‚Â Â  â””â”€â”€ train
â”‚Â Â  â”‚Â Â      â””â”€â”€ drone
â”‚Â Â  â”œâ”€â”€ 2fps
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ gallery_drone
â”‚Â Â  â”‚Â Â  â””â”€â”€ train
â”‚Â Â  â”‚Â Â      â””â”€â”€ drone
â”‚Â Â  â””â”€â”€ 5fps
â”‚Â Â      â”œâ”€â”€ test
â”‚Â Â      â”‚Â Â  â””â”€â”€ gallery_drone
â”‚Â Â      â””â”€â”€ train
â”‚Â Â          â””â”€â”€ drone
â”œâ”€â”€ 45
â”‚Â Â  â”œâ”€â”€ 10fps
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ gallery_drone
â”‚Â Â  â”‚Â Â  â””â”€â”€ train
â”‚Â Â  â”‚Â Â      â””â”€â”€ drone
â”‚Â Â  â”œâ”€â”€ 2fps
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gallery_drone
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gallery_satellite
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ gallery_street
â”‚Â Â  â”‚Â Â  â””â”€â”€ train
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ drone
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ google
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ satellite
â”‚Â Â  â”‚Â Â      â””â”€â”€ street
â”‚Â Â  â””â”€â”€ 5fps
â”‚Â Â      â”œâ”€â”€ test
â”‚Â Â      â”‚Â Â  â””â”€â”€ gallery_drone
â”‚Â Â      â””â”€â”€ train
â”‚Â Â          â””â”€â”€ drone
â”œâ”€â”€ dataset_split.json
â””â”€â”€ organize_univ.py
```

We note that there are no overlaps between 33 univeristies of training set and 39 univeristies of test set.

## Getting started

### Installation

```bash
conda create --name video2bev python=3.7
# pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
pip install -r requirements.txt

# (optional but recommended) install apex
git clone https://github.com/NVIDIA/apex.git
cd apex
python setup.py install --cuda_ext --cpp_ext
```

If you have any question of installing apex, please refer to [issue-2](https://github.com/HaoDot/Video2BEV-Open/issues/2) first, then search for possible solutions.

## Dataset & Preparation
- Download UniV.

- cat and unzip the dataset: `cat UniV.tar.xz.* | tar -xvJf - --transform 's|.*/|UniV/|'`

- [Optional] If you are interested in reproducing or evaluating the proposed Video2BEV, please feel free to contact us and ask for **BEVs** and **synthetic negative samples**.
- [Optional] If you are interested in the proposed Video2BEV Transformation, please feel free to contact us and ask for **SFM** and **3DGS** outputs.

## Training & Evaluation

### Training

#### First-stage training & evaluation

- First-stage training:
  - Check to `first-stage` branch by `git checkout first-stage`
  - Refer to [this file](https://github.com/HaoDot/Video2BEV-Open/tree/first-stage)
- First-stage evaluation:
  - Check to `first-stage` branch by `git checkout first-stage`
  - Refer to [this file](https://github.com/HaoDot/Video2BEV-Open/tree/first-stage)

```bash
# Train:
# In the first stage, we fine-tune the encoder with the instance loss and contrastive loss.
sh train.sh
# Evaluation:
python test_collect_weights.py;
sh test.sh
```

####  Second-stage training & evaluation

- Second-stage training:
  - Check to `second-stage-training` branch by `git checkout second-stage-training`
  - Refer to [this file](https://github.com/HaoDot/Video2BEV-Open/tree/second-stage-training)
- Second-stage evaluation:
  - Check to `second-stage-evalution` branch by `git checkout second-stage-evalution`
  - Refer to [this file](https://github.com/HaoDot/Video2BEV-Open/blob/second-stage-evalution/README.md)

```bash
# Train:
# In the first stage, we fine-tune the encoder with the instance loss and contrastive loss.
# please change contents in train.sh
sh train.sh

# Evaluation:
# please change contents in test_collect_weights.py and test.sh
python test_collect_weights.py;
sh test.sh
```

## Weights

[Download link](https://pan.baidu.com/s/1ZjssipR0RGfoPaETo4QhsQ?pwd=ahi4)

```bash
.
â”œâ”€â”€ first-stage
â”‚Â Â  â”œâ”€â”€ 30-degree
â”‚Â Â  â”‚Â Â  â””â”€â”€ model_xxxx_xxxx
â”‚Â Â  â”‚Â Â      â””â”€â”€ two_view_long_share_d0.75_256_s1
â”‚Â Â  â”‚Â Â          â””â”€â”€ model_xxxx_xxxx_xxx
â”‚Â Â  â”‚Â Â              â”œâ”€â”€ net_9301.pth
â”‚Â Â  â”‚Â Â              â””â”€â”€ opts.yaml
â”‚Â Â  â””â”€â”€ 45-degree
â”‚Â Â      â””â”€â”€ model_2024-08-20-19_19_36
â”‚Â Â          â””â”€â”€ two_view_long_share_d0.75_256_s1
â”‚Â Â              â””â”€â”€ model_2024-08-20-19_19_36_059
â”‚Â Â                  â”œâ”€â”€ net_059.pth
â”‚Â Â                  â””â”€â”€ opts.yaml
â”œâ”€â”€ second-stage
â”‚Â Â  â”œâ”€â”€ 30degree-2fps
â”‚Â Â  â”‚Â Â  â””â”€â”€ model_2024-11-02-03-05-31.zip
â”‚Â Â  â”œâ”€â”€ 45degree-2fps
â”‚Â Â  â”‚Â Â  â””â”€â”€ model_2024-10-05-02_49_11.zip
â”‚Â Â  â””â”€â”€ 45degree-2fps-better
â”‚Â Â      â””â”€â”€ model_2024-10-20-06_02_09.zip
â””â”€â”€ vit_small_p16_224-15ec54c9.pth
```

Choose the weight and **unzip** it. Then put it in the root path in the working directory for your repo.

PS: 

- `model_2024-11-02-03-05-31` is the weight for 30-degree UniV (2fps) and `model_2024-10-05-02_49_11` is the weight for 45-degree UniV (2fps)
  - The evaluation number should be the same as our paper
- By tuning hyper-parameter, we can get a better result.



------

## Citation

The following paper uses and reports the result of the baseline model. You may cite it in your paper.
```bibtex
@article{ju2024video2bev,
  title={Video2bev: Transforming drone videos to bevs for video-based geo-localization},
  author={Ju, Hao and Huang, Shaofei and Liu, Si and Zheng, Zhedong},
  journal={arXiv preprint arXiv:2411.13610},
  year={2024}
}
```
Others:

```bibtex
@article{zheng2020university,
  title={University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization},
  author={Zheng, Zhedong and Wei, Yunchao and Yang, Yi},
  journal={ACM Multimedia},
  year={2020}
}
@article{zheng2017dual,
  title={Dual-Path Convolutional Image-Text Embeddings with Instance Loss},
  author={Zheng, Zhedong and Zheng, Liang and Garrett, Michael and Yang, Yi and Xu, Mingliang and Shen, Yi-Dong},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  doi={10.1145/3383184},
  volume={16},
  number={2},
  pages={1--23},
  year={2020},
  publisher={ACM New York, NY, USA}
}
```

